{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBdKGD39S58T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import random\n",
    "import os.path as osp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "CSFF51DrTFQb"
   },
   "outputs": [],
   "source": [
    "LABEL_PATH = \"../../data/modified_labels_compass/\"\n",
    "VID_PATH = \"../../data/Walking_with_compass/\"\n",
    "PROCESSED_PATH = \"../../data/processed\"\n",
    "DATA_SAVE_PATH = \"../../data/videos_modified_augmented\"\n",
    "MODELS_PATHS = \"./models\"\n",
    "FRAME_RATE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "iUspG4_on3ax"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Preprocess video data.\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "def map_to_multiclass(lab):\n",
    "    if lab == 'LEFT':\n",
    "        return 0\n",
    "    if lab == 'RIGHT':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def get_all_files_from_dir(directory, basename =None):\n",
    "    file_paths = []\n",
    "    # print(directory)\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            files = [f for f in files if not f[0] == '.']\n",
    "            if basename is None:\n",
    "                file_paths += [os.path.join(root, x) for x in files]\n",
    "            else:\n",
    "                file_paths += [os.path.join(root, x) for x in files if x.startswith(basename)]\n",
    "        return sorted(file_paths)\n",
    "    except Exception as e:\n",
    "        print(\"Error: \",e)\n",
    "    \n",
    "def get_lab(labels, time):\n",
    "    for row in labels:\n",
    "        if time <= float(row[2]) and time >= float(row[1]):\n",
    "            return row[0]\n",
    "\n",
    "def get_length(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)\n",
    "\n",
    "def inRange(a, b, c, mode = 'full'):\n",
    "    q = round(a/1000 , 3)\n",
    "    if(mode == 'full'):\n",
    "        if (q>=b and q<=c):\n",
    "            return True\n",
    "    else:\n",
    "        if(q>=b):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def searchRowConstraint(df_series, a):\n",
    "    return df_series.searchsorted(a, side='left')\n",
    "\n",
    "def uniformSampling(lst,vids):\n",
    "    # [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, \n",
    "    # 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    if(len(lst)<=20):\n",
    "        return lst,vids\n",
    "    m = len(lst)//20\n",
    "    \n",
    "    new_lst = []\n",
    "    new_vids = []\n",
    "    \n",
    "    lst.reverse()\n",
    "    vids.reverse()\n",
    "    \n",
    "    for i in range(0,len(lst),m):\n",
    "        new_lst.append(lst[i])\n",
    "        new_vids.append(vids[i])\n",
    "        \n",
    "    new_lst.reverse()\n",
    "    new_vids.reverse()\n",
    "    \n",
    "    return new_lst, new_vids\n",
    "\n",
    "def process_video(video_file, label_filepath):\n",
    "    video_filename = video_file.split('/')[-1].split('.')[0]\n",
    "    label_files = get_all_files_from_dir(label_filepath, video_filename)\n",
    "    labels_meta = []\n",
    "    video_labels = {}\n",
    "    # video_pos = {}\n",
    "    total_labels = 0\n",
    "\n",
    "    for l_files  in label_files:\n",
    "        print(l_files)\n",
    "        labels = pd.read_csv(l_files,sep='\\t',header=None)\n",
    "        total_labels += len(labels)\n",
    "        labels[1] = labels[1]-1\n",
    "        labels[2] = labels[2]-1\n",
    "        labels[0] = labels[0].apply(map_to_multiclass)\n",
    "        l_files = l_files.split('/')[-1].split('.')[0]\n",
    "        labels[3] = l_files\n",
    "\n",
    "        pos = labels[0].ne(2).idxmax()\n",
    "        # print(\"Pos:\",pos)\n",
    "        pos_start = searchRowConstraint(labels[1], labels.iat[pos,1]-3)\n",
    "        # print(\"Pos Start:\",pos_start)\n",
    "        pos_end = searchRowConstraint(labels[1], labels.iat[pos,2]+3)\n",
    "        # print(\"Pos Range:\",pos_start,\", \",pos_end)\n",
    "        temp = labels[pos_start:pos_end]\n",
    "        temp[3] = l_files+\"_mid\"\n",
    "        labels_meta.append(temp)\n",
    "        video_labels[l_files+\"_mid\"] = {\"frames\":[],\"labels\":[]}\n",
    "        video_labels[l_files+\"_mid_flip\"] = {\"frames\":[],\"labels\":[]}\n",
    "        \n",
    "        if(pos_end < len(labels)):\n",
    "            pos_start_right = searchRowConstraint(labels[1], labels.iat[pos_end,2]+2)\n",
    "            if(pos_start_right < len(labels)):\n",
    "                pos_end_right = searchRowConstraint(labels[1], labels.iat[pos_start_right,2]+6)\n",
    "                if(pos_start_right <= pos_end_right - 3): \n",
    "                    temp = labels[pos_start_right:pos_end_right]\n",
    "                    temp[3] = l_files+\"_right\"\n",
    "                    labels_meta.append(temp)\n",
    "                    video_labels[l_files+\"_right\"] = {\"frames\":[],\"labels\":[]}\n",
    "                    video_labels[l_files+\"_right_flip\"] = {\"frames\":[],\"labels\":[]}\n",
    "\n",
    "        \n",
    "        if(pos_start > 0):\n",
    "            pos_start_left = searchRowConstraint(labels[1], labels.iat[pos_start,1]-7)\n",
    "            pos_end_left = searchRowConstraint(labels[2], labels.iat[pos_start,2]-1)\n",
    "            if(pos_start_left <= pos_end_left - 3): \n",
    "                temp = labels[pos_start_left:pos_end_left]\n",
    "                temp[3] = l_files+\"_left\"\n",
    "                labels_meta.append(temp)\n",
    "                video_labels[l_files+\"_left\"] = {\"frames\":[],\"labels\":[]}\n",
    "                video_labels[l_files+\"_left_flip\"] = {\"frames\":[],\"labels\":[]}\n",
    "    \n",
    "    labels = pd.concat(labels_meta)\n",
    "    labels = labels.sort_values(1)\n",
    "    labels.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # labels.to_csv('labels.csv',index=None)\n",
    "    # return \n",
    "    \n",
    "    \n",
    "    labels = labels.to_numpy()\n",
    "    # print(labels)\n",
    "    # return\n",
    "\n",
    "    vidcap = cv2.VideoCapture(video_file)\n",
    "    # fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    ctr = 0\n",
    "    lbl = 0\n",
    "    \n",
    "    row = labels[lbl]\n",
    "    hasFrames,image = vidcap.read()\n",
    "    fl = 0\n",
    "    \n",
    "    while(True): \n",
    "        \n",
    "        while (hasFrames and not inRange(vidcap.get(cv2.CAP_PROP_POS_MSEC), float(row[1]), float(row[2]), mode='half')):\n",
    "            hasFrames,image = vidcap.read()\n",
    "            # print(vidcap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "        while(hasFrames and inRange(vidcap.get(cv2.CAP_PROP_POS_MSEC), float(row[1]),float(row[2]))):\n",
    "            # print(\"Row: \",lbl)\n",
    "            try:\n",
    "                \n",
    "                save_file_name = row[3] + \"_\" + str(ctr) + \".npy\"\n",
    "                np.save(osp.join(PROCESSED_PATH, save_file_name), image)\n",
    "                if(int(row[0]) == 1):\n",
    "                    fl = 0\n",
    "                elif(int(row[0]) == 0):\n",
    "                    fl = 1\n",
    "                else:\n",
    "                    fl = 2\n",
    "                \n",
    "                save_file_name_flipped = row[3]+\"_flip\" + \"_\" + str(ctr) + \".npy\"\n",
    "                np.save(osp.join(PROCESSED_PATH, save_file_name_flipped),  cv2.flip(image, 1))\n",
    "                \n",
    "                video_labels[row[3]]['labels'].append(int(row[0]))\n",
    "                video_labels[row[3]]['frames'].append(save_file_name)\n",
    "                video_labels[row[3]+\"_flip\"]['labels'].append(fl)\n",
    "                video_labels[row[3]+\"_flip\"]['frames'].append(save_file_name_flipped)\n",
    "                ctr += 1\n",
    "                for _ in range(2):\n",
    "                    hasFrames,image = vidcap.read()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                hasFrames,image = vidcap.read()\n",
    "            \n",
    "    \n",
    "        if(hasFrames == False or lbl >= len(labels)-1):\n",
    "            print(\"Has Frames: \", hasFrames)\n",
    "            break\n",
    "\n",
    "        lbl += 1\n",
    "        row = labels[lbl]\n",
    "   \n",
    "    frame_count = 0\n",
    "    for k,v in video_labels.items():\n",
    "        v['labels'],v['frames'] = uniformSampling(v['labels'],v['frames'])\n",
    "        # print(len(v['labels']))\n",
    "        save_file = k + \".csv\"\n",
    "        df = pd.DataFrame(v)\n",
    "        # print(df.head())\n",
    "        df.to_csv(osp.join(DATA_SAVE_PATH,save_file), index=None)\n",
    "        frame_count += len(v['frames'])\n",
    "\n",
    "    print(\"After processing:\")\n",
    "    print(\"Length of labels: \",total_labels)\n",
    "    print(\"Labels utilized: \", lbl)\n",
    "    print(\"Frames labeled: \", frame_count)\n",
    "    \n",
    "def preprocess():\n",
    "    for video_filename in get_all_files_from_dir(VID_PATH):\n",
    "        process_video(video_filename, LABEL_PATH)\n",
    "        print(\"Finished processing \", video_filename)\n",
    "        # return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/modified_labels_compass/walking_data_1_a.csv\n",
      "../../data/modified_labels_compass/walking_data_1_b.csv\n",
      "../../data/modified_labels_compass/walking_data_1_c.csv\n",
      "../../data/modified_labels_compass/walking_data_1_d.csv\n",
      "../../data/modified_labels_compass/walking_data_1_e.csv\n",
      "../../data/modified_labels_compass/walking_data_1_f.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4077/2400670210.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[3] = l_files+\"_mid\"\n",
      "/tmp/ipykernel_4077/2400670210.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[3] = l_files+\"_right\"\n",
      "/tmp/ipykernel_4077/2400670210.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp[3] = l_files+\"_left\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  405\n",
      "Labels utilized:  285\n",
      "Frames labeled:  482\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_1.mp4\n",
      "../../data/modified_labels_compass/walking_data_2_a.csv\n",
      "../../data/modified_labels_compass/walking_data_2_b.csv\n",
      "../../data/modified_labels_compass/walking_data_2_c.csv\n",
      "../../data/modified_labels_compass/walking_data_2_d.csv\n",
      "../../data/modified_labels_compass/walking_data_2_e.csv\n",
      "../../data/modified_labels_compass/walking_data_2_f.csv\n",
      "../../data/modified_labels_compass/walking_data_2_g.csv\n",
      "../../data/modified_labels_compass/walking_data_2_h.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  511\n",
      "Labels utilized:  371\n",
      "Frames labeled:  588\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_2.mp4\n",
      "../../data/modified_labels_compass/walking_data_3_a.csv\n",
      "../../data/modified_labels_compass/walking_data_3_b.csv\n",
      "../../data/modified_labels_compass/walking_data_3_c.csv\n",
      "../../data/modified_labels_compass/walking_data_3_d.csv\n",
      "../../data/modified_labels_compass/walking_data_3_e.csv\n",
      "../../data/modified_labels_compass/walking_data_3_f.csv\n",
      "../../data/modified_labels_compass/walking_data_3_g.csv\n",
      "../../data/modified_labels_compass/walking_data_3_h.csv\n",
      "../../data/modified_labels_compass/walking_data_3_i.csv\n",
      "../../data/modified_labels_compass/walking_data_3_j.csv\n",
      "../../data/modified_labels_compass/walking_data_3_k.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  409\n",
      "Labels utilized:  345\n",
      "Frames labeled:  678\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_3.mp4\n",
      "../../data/modified_labels_compass/walking_data_4_a.csv\n",
      "../../data/modified_labels_compass/walking_data_4_b.csv\n",
      "../../data/modified_labels_compass/walking_data_4_c.csv\n",
      "../../data/modified_labels_compass/walking_data_4_d.csv\n",
      "../../data/modified_labels_compass/walking_data_4_e.csv\n",
      "../../data/modified_labels_compass/walking_data_4_f.csv\n",
      "../../data/modified_labels_compass/walking_data_4_g.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  439\n",
      "Labels utilized:  322\n",
      "Frames labeled:  490\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_4.mp4\n",
      "../../data/modified_labels_compass/walking_data_5_a.csv\n",
      "../../data/modified_labels_compass/walking_data_5_b.csv\n",
      "../../data/modified_labels_compass/walking_data_5_c.csv\n",
      "../../data/modified_labels_compass/walking_data_5_d.csv\n",
      "../../data/modified_labels_compass/walking_data_5_e.csv\n",
      "../../data/modified_labels_compass/walking_data_5_f.csv\n",
      "../../data/modified_labels_compass/walking_data_5_g.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  383\n",
      "Labels utilized:  280\n",
      "Frames labeled:  420\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_5.mp4\n",
      "../../data/modified_labels_compass/walking_data_6_a.csv\n",
      "../../data/modified_labels_compass/walking_data_6_b.csv\n",
      "../../data/modified_labels_compass/walking_data_6_c.csv\n",
      "../../data/modified_labels_compass/walking_data_6_d.csv\n",
      "../../data/modified_labels_compass/walking_data_6_e.csv\n",
      "../../data/modified_labels_compass/walking_data_6_f.csv\n",
      "../../data/modified_labels_compass/walking_data_6_g.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  414\n",
      "Labels utilized:  305\n",
      "Frames labeled:  430\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_6.mp4\n",
      "../../data/modified_labels_compass/walking_data_7_a.csv\n",
      "../../data/modified_labels_compass/walking_data_7_b.csv\n",
      "../../data/modified_labels_compass/walking_data_7_c.csv\n",
      "../../data/modified_labels_compass/walking_data_7_d.csv\n",
      "../../data/modified_labels_compass/walking_data_7_e.csv\n",
      "../../data/modified_labels_compass/walking_data_7_f.csv\n",
      "../../data/modified_labels_compass/walking_data_7_g.csv\n",
      "../../data/modified_labels_compass/walking_data_7_h.csv\n",
      "../../data/modified_labels_compass/walking_data_7_i.csv\n",
      "../../data/modified_labels_compass/walking_data_7_j.csv\n",
      "../../data/modified_labels_compass/walking_data_7_k.csv\n",
      "../../data/modified_labels_compass/walking_data_7_l.csv\n",
      "../../data/modified_labels_compass/walking_data_7_m.csv\n",
      "Has Frames:  True\n",
      "After processing:\n",
      "Length of labels:  411\n",
      "Labels utilized:  348\n",
      "Frames labeled:  1032\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_7.mp4\n"
     ]
    }
   ],
   "source": [
    "### preprocess videos\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "e5gsuYg0EDMN"
   },
   "outputs": [],
   "source": [
    "BATCH = 1\n",
    "# SEQUENCE_LENGTH = 20\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "IIFM_Mc9Tjnz"
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, files, transforms, base_path):\n",
    "        self.transforms = transforms\n",
    "        self.files = files\n",
    "        self.base_path = base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_filename = self.files[idx]\n",
    "        # print(\"Data point: \",seq_filename) \n",
    "        data = pd.read_csv(seq_filename)\n",
    "        # print(data.head())\n",
    "        video = torch.FloatTensor(len(data), CHANNELS, HEIGHT, WIDTH)\n",
    "        \n",
    "        for e,filename in enumerate(data['frames']):\n",
    "            try:\n",
    "                # print(filename)\n",
    "                frame = np.load(osp.join(self.base_path,filename), allow_pickle=True)\n",
    "                frame = (frame - frame.min())/(frame.max() - frame.min())\n",
    "                frame = self.transforms(frame)\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                frame = torch.zeros((CHANNELS, HEIGHT, WIDTH))\n",
    "\n",
    "            video[e,:,:,:] = frame\n",
    "          \n",
    "        return video, torch.LongTensor(data['labels'].to_numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tt_split(data_folder):\n",
    "    X = []\n",
    "    y = []\n",
    "    n = 0\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if(filename[-3:]==\"csv\"):\n",
    "            X.append(osp.join(data_folder,filename))\n",
    "            # df = pd.read_csv(osp.join(data_folder,filename))\n",
    "            # n += 1\n",
    "            # for i in range(len(df)-seq_len):\n",
    "            #     X.append(df['frames'][i:i+seq_len].tolist())\n",
    "            #     y.append(df['labels'][i:i+seq_len].tolist())\n",
    "    # print(X)\n",
    "    random.shuffle(X)\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    print(\"Train size: \",train_size)\n",
    "    return X[:train_size], X[train_size:]\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    # print(X_train)\n",
    "    # print(n)\n",
    "    # print(X_test)\n",
    "    # return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xNuSoj1BLWea",
    "outputId": "a1e98128-490a-4651-c60f-ef4499290378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can also explore https://github.com/okankop/vidaug for video based augmentations.\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can also explore https://github.com/okankop/vidaug for video based augmentations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IX1zg-g3In-9",
    "outputId": "aff9d5b2-0a91-4ecc-b6b2-9beaed1448aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Train size:  144\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# train_transforms = [ttf.ToTensor(), transforms.Resize((HEIGHT, WIDTH)), transforms.ColorJitter(), transforms.RandomRotation(10), transforms.GaussianBlur(3)]\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((HEIGHT, WIDTH))])\n",
    "val_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((HEIGHT, WIDTH))])\n",
    "\n",
    "X_train, X_test = make_tt_split(DATA_SAVE_PATH)\n",
    "train_dataset = VideoDataset(files = X_train, transforms=train_transforms,base_path = PROCESSED_PATH)\n",
    "val_dataset = VideoDataset(files = X_test, transforms=val_transforms, base_path = PROCESSED_PATH)\n",
    "\n",
    "\n",
    "train_args = dict(shuffle=True, batch_size=BATCH, num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=True, batch_size=BATCH, drop_last=False)\n",
    "train_loader = DataLoader(train_dataset, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=BATCH, num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=BATCH, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, **val_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tgnvcg7a8hHp",
    "outputId": "a4373e9f-1c67-477f-d9a7-4f3088d160cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "b5YXDoksGA_i"
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h) #[batch_size, self.hidden_dim, height, width]\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1) #[batch_size,t,self.hidden_dim, height, width]\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "9_s93Au3Rk-F"
   },
   "outputs": [],
   "source": [
    "class ConvLSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False, num_classes = 3):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers,batch_first, bias, return_all_layers)\n",
    "        self.linear = nn.Linear(hidden_dim * HEIGHT * WIDTH, num_classes)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "      x,_ = self.convlstm(input_tensor)\n",
    "      # print(x[0].shape)  # torch.Size([2, 8, 128, 256, 256])\n",
    "      x = torch.flatten(x[0], start_dim=2)\n",
    "      # print(x.shape)  \t# torch.Size([2, 8, 8388608])\n",
    "      x = self.linear(x) #op: [batch, t, num_classes]\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "UrI2yNxqbvvI"
   },
   "outputs": [],
   "source": [
    "def save(model, index, optim = False):\n",
    "    if not os.path.exists(MODELS_PATHS+'/attempt5_1sec_prior_labels_modified_data'):\n",
    "        os.mkdir(MODELS_PATHS+'/attempt5_1sec_prior_labels_modified_data')\n",
    "    if(optim):\n",
    "        torch.save(model.state_dict(), MODELS_PATHS+'/attempt5_1sec_prior_labels_modified_data'+'/optimizer_params_{:08d}.pth'.format(index))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), MODELS_PATHS+'/attempt5_1sec_prior_labels_modified_data'+'/model_params_{:08d}.pth'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sl89a6wJMJEA",
    "outputId": "dbe36a2f-f49a-44f4-e87e-ddb88ab225bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTMModel(\n",
      "  (convlstm): ConvLSTM(\n",
      "    (cell_list): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (conv): Conv2d(131, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): ConvLSTMCell(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2097152, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005 #changed from 0.01\n",
    "epochs = 50\n",
    "lamda = 1e-2  #L2 regularization #changed from 1e-4\n",
    "num_classes = 3\n",
    "convlstm_hidden = 128\n",
    "num_conv_lstm_layers = 2\n",
    "\n",
    "model = ConvLSTMModel(CHANNELS,convlstm_hidden,(3,3),num_conv_lstm_layers,True)\n",
    "model.load_state_dict(torch.load('./models/attempt5_1sec_prior_labels_modified_data/model_params_00000044.pth'))  #Validation: 75.4808%\n",
    "model = model.to(device)\n",
    "\n",
    "# class_weights = [2,2,1] \n",
    "# class_weights = torch.Tensor(class_weights)\n",
    "# class_weights = class_weights.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight =class_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lamda, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamda)\n",
    "optimizer.load_state_dict(torch.load('./models/attempt5_1sec_prior_labels_modified_data/optimizer_params_00000044.pth')) #Validation: 75.4808%\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr\n",
    "    g['weight_decay']= lamda\n",
    "    \n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "dhrrwnCikFAG",
    "outputId": "fb495db9-61a9-4511-b30c-ea4144f8004e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/50: Train Acc 87.7129%, Train Loss 5.3651, Learning Rate 0.0005\n",
      "Validation: 70.6731%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/50: Train Acc 92.3662%, Train Loss 3.2925, Learning Rate 0.0005\n",
      "Validation: 77.4038%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/50: Train Acc 93.1873%, Train Loss 2.6006, Learning Rate 0.0005\n",
      "Validation: 71.9952%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/50: Train Acc 94.1910%, Train Loss 2.0015, Learning Rate 0.0005\n",
      "Validation: 64.1827%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/50: Train Acc 92.5182%, Train Loss 2.3271, Learning Rate 0.0005\n",
      "Validation: 62.7404%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/50: Train Acc 93.1569%, Train Loss 2.0251, Learning Rate 0.0005\n",
      "Validation: 68.3894%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/50: Train Acc 91.6058%, Train Loss 2.8741, Learning Rate 0.0005\n",
      "Validation: 71.3942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/50: Train Acc 92.9440%, Train Loss 2.8910, Learning Rate 0.0005\n",
      "Validation: 70.0721%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/50: Train Acc 91.7579%, Train Loss 3.0212, Learning Rate 0.0005\n",
      "Validation: 66.2260%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/50: Train Acc 92.3054%, Train Loss 2.7666, Learning Rate 0.0005\n",
      "Validation: 67.7885%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/50: Train Acc 89.8418%, Train Loss 3.9781, Learning Rate 0.0004\n",
      "Validation: 54.9279%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/50: Train Acc 85.7968%, Train Loss 8.3974, Learning Rate 0.0004\n",
      "Validation: 75.4808%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/50: Train Acc 92.8528%, Train Loss 2.2270, Learning Rate 0.0004\n",
      "Validation: 66.9471%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/50: Train Acc 93.2482%, Train Loss 1.5522, Learning Rate 0.0004\n",
      "Validation: 59.6154%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/50: Train Acc 92.3054%, Train Loss 2.2396, Learning Rate 0.0004\n",
      "Validation: 67.5481%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/50: Train Acc 95.4988%, Train Loss 1.2859, Learning Rate 0.0004\n",
      "Validation: 74.8798%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/50: Train Acc 93.7652%, Train Loss 2.0809, Learning Rate 0.0004\n",
      "Validation: 69.1106%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/50: Train Acc 93.8564%, Train Loss 1.6828, Learning Rate 0.0004\n",
      "Validation: 71.2740%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/50: Train Acc 94.8905%, Train Loss 1.1969, Learning Rate 0.0003\n",
      "Validation: 68.9904%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/50: Train Acc 87.2567%, Train Loss 6.5776, Learning Rate 0.0003\n",
      "Validation: 47.2356%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  74%|████████████████████████▌        | 107/144 [01:12<00:25,  1.43it/s, acc=92.1689%, loss=2.3730, lr=0.0003, num_correct=2248]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4077/3524361935.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mframes_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mframes_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p38/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50,50+epochs):\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    frames_count = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # print(x.shape)\n",
    "        frames_count += x.shape[1]\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(x)\n",
    "            del x\n",
    "            loss = criterion(outputs.view(-1,num_classes), y.long().view(-1))\n",
    "\n",
    "        num_correct += int((torch.argmax(outputs, axis=2) == y).sum())\n",
    "        del outputs\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * num_correct / frames_count),\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            num_correct=num_correct,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "        \n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "        epoch + 1,\n",
    "        epochs,\n",
    "        100 * num_correct / frames_count,\n",
    "        float(total_loss / len(train_loader)),\n",
    "        float(optimizer.param_groups[0]['lr'])))\n",
    "    \n",
    "    save(model, epoch)\n",
    "    save(optimizer, epoch, optim=True)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_num_correct = 0\n",
    "    frames_count = 0\n",
    "    \n",
    "    for i, (vx, vy) in enumerate(val_loader):\n",
    "      \n",
    "        vx = vx.to(device)\n",
    "        vy = vy.to(device)\n",
    "        frames_count += vx.shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(vx)\n",
    "            del vx\n",
    "\n",
    "        val_num_correct += int((torch.argmax(outputs, axis=2) == vy).sum())\n",
    "        del outputs\n",
    "\n",
    "    print(\"Validation: {:.04f}%\".format(100 * val_num_correct / frames_count))\n",
    "\n",
    "    \n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 65.3846%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AI_Guide_Dog_Training_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
