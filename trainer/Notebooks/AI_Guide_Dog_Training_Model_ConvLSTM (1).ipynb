{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mBdKGD39S58T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CSFF51DrTFQb"
   },
   "outputs": [],
   "source": [
    "VID_PATH = \"../../data/Walking_with_compass/\"\n",
    "LABEL_PATH = \"../../data/compass/\"\n",
    "PROCESSED_PATH = \"../../data/processed\"\n",
    "DATA_SAVE_PATH = \"../../data/videos\"\n",
    "MODELS_PATHS = \"./models\"\n",
    "FRAME_RATE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iUspG4_on3ax"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Preprocess video data.\n",
    "\"\"\"\n",
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "def map_to_multiclass(lab):\n",
    "    if lab == 'LEFT':\n",
    "        return 0\n",
    "    if lab == 'RIGHT':\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def get_all_files_from_dir(directory):\n",
    "    file_paths = []\n",
    "    print(directory)\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            print(files)\n",
    "            file_paths += [os.path.join(root, x) for x in files]\n",
    "        return sorted(file_paths)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "def get_lab(labels, time):\n",
    "    for row in labels:\n",
    "        if time <= float(row[2]) and time >= float(row[1]):\n",
    "            return row[0]\n",
    "\n",
    "def get_length(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)\n",
    "\n",
    "def inRange(a, b, c, mode = 'full'):\n",
    "    q = round(a/1000 , 3)\n",
    "    if(mode == 'full'):\n",
    "        if (q>=b and q<=c):\n",
    "            return True\n",
    "    else:\n",
    "        if(q>=b):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def process_video(video_file, label_filename):\n",
    "    video_filename = video_file.split('/')[-1].split('.')[0]\n",
    "    # file_num = video_filename.split('/')[-1].split('.')[0].split(\"_\")[-1]\n",
    "\n",
    "    #       with open(label_filename, 'r') as f:\n",
    "    #           labels_str = f.read()\n",
    "\n",
    "    #       labels = [row.split('\\t') for row in labels_str.split('\\n')]\n",
    "\n",
    "    labels = pd.read_csv(label_filename, sep='\\t', header=None)\n",
    "    labels[1] = labels[1]-1\n",
    "    labels[2] = labels[2]-1\n",
    "    labels[0] = labels[0].apply(map_to_multiclass)\n",
    "    labels = labels.to_numpy()\n",
    "#     print(labels)\n",
    "   \n",
    "    vidcap = cv2.VideoCapture(video_file)\n",
    "    # fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    ctr = 0\n",
    "    lbl = 0\n",
    "    \n",
    "    row = labels[lbl]\n",
    "    hasFrames,image = vidcap.read()\n",
    "    # print(row[0],row[1],row[2])\n",
    "  # while(hasFrames):\n",
    "  #   print(vidcap.get(cv2.CAP_PROP_POS_MSEC)/1000)\n",
    "  #   hasFrames,image = vidcap.read()\n",
    "  # print(\"+++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "  # return\n",
    "\n",
    "\n",
    "    while (hasFrames and not inRange(vidcap.get(cv2.CAP_PROP_POS_MSEC), float(row[1]), float(row[2]), mode='half')):\n",
    "        hasFrames,image = vidcap.read()\n",
    "\n",
    "\n",
    "    video_frames = []\n",
    "    video_labels = []\n",
    "    while(True): \n",
    "        try:\n",
    "            while(hasFrames and inRange(vidcap.get(cv2.CAP_PROP_POS_MSEC), float(row[1]),float(row[2]))):\n",
    "                video_labels.append(int(row[0]))\n",
    "                # savefile = {'image': image_to_save, 'label': label_to_save}\n",
    "                save_file_name = video_filename + \"_\" + str(ctr) + \".npy\"\n",
    "                video_frames.append(save_file_name)\n",
    "                np.save(osp.join(PROCESSED_PATH, save_file_name), image)\n",
    "                ctr += 1\n",
    "                for _ in range(2):\n",
    "                    hasFrames,image = vidcap.read()\n",
    "        except Exception as e:\n",
    "            print(\"Error occured 1: \",e)\n",
    "\n",
    "        if(hasFrames == False or lbl >= len(labels)-1):\n",
    "            break\n",
    "\n",
    "        lbl += 1\n",
    "        row = labels[lbl]\n",
    "    \n",
    "    df = pd.DataFrame({'frames': video_frames, 'labels': video_labels})\n",
    "    df.to_csv(osp.join(DATA_SAVE_PATH,video_filename+\".csv\"), index=None)\n",
    "\n",
    "    print(\"After processing:\")\n",
    "    print(\"Length of labels: \",len(labels))\n",
    "    print(\"Labels utilized: \",lbl)\n",
    "    print(\"Frames labeled: \", len(video_frames))\n",
    "    \n",
    "def preprocess():\n",
    "    for video_filename, label_filename in zip(get_all_files_from_dir(VID_PATH), get_all_files_from_dir(LABEL_PATH)):\n",
    "        process_video(video_filename, label_filename)\n",
    "        print(\"Finished processing \", video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "Wt9bcSU7dReN",
    "outputId": "9d0484db-59c3-4c2c-b515-ab43df7f9a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/Walking_with_compass/\n",
      "['walking_data_3.mp4', 'walking_data_7.mp4', 'walking_data_2.mp4', 'walking_data_4.mp4', 'walking_data_1.mp4', 'walking_data_6.mp4', 'walking_data_5.mp4']\n",
      "../../data/compass/\n",
      "['walking_data_6_compass_label.txt', 'walking_data_2_compass_label.txt', 'walking_data_7_compass_label.txt', 'walking_data_3_compass_label.txt', 'walking_data_4_compass_label.txt', 'walking_data_1_compass_label.txt', 'walking_data_5_compass_label.txt']\n",
      "After processing:\n",
      "Length of labels:  405\n",
      "Labels utilized:  404\n",
      "Frames labeled:  656\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_1.mp4\n",
      "After processing:\n",
      "Length of labels:  511\n",
      "Labels utilized:  510\n",
      "Frames labeled:  812\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_2.mp4\n",
      "After processing:\n",
      "Length of labels:  409\n",
      "Labels utilized:  408\n",
      "Frames labeled:  720\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_3.mp4\n",
      "After processing:\n",
      "Length of labels:  439\n",
      "Labels utilized:  438\n",
      "Frames labeled:  686\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_4.mp4\n",
      "After processing:\n",
      "Length of labels:  383\n",
      "Labels utilized:  382\n",
      "Frames labeled:  650\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_5.mp4\n",
      "After processing:\n",
      "Length of labels:  414\n",
      "Labels utilized:  413\n",
      "Frames labeled:  620\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_6.mp4\n",
      "After processing:\n",
      "Length of labels:  1024\n",
      "Labels utilized:  1023\n",
      "Frames labeled:  2716\n",
      "Finished processing  ../../data/Walking_with_compass/walking_data_7.mp4\n"
     ]
    }
   ],
   "source": [
    "### preprocess videos\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e5gsuYg0EDMN"
   },
   "outputs": [],
   "source": [
    "BATCH = 2\n",
    "SEQUENCE_LENGTH = 10\n",
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IIFM_Mc9Tjnz"
   },
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, x, y, transforms, seq_len, base_path):\n",
    "        self.transforms = transforms\n",
    "        self.X = x\n",
    "        self.y = y\n",
    "        self.seq_len = seq_len\n",
    "        self.base_path = base_path\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_filename = self.X[idx]\n",
    "        video = torch.FloatTensor(self.seq_len, CHANNELS, HEIGHT, WIDTH)\n",
    "        for e,filename in enumerate(seq_filename):\n",
    "            try:\n",
    "                frame = np.load(osp.join(self.base_path,filename), allow_pickle=True)\n",
    "                frame = (frame - frame.min())/(frame.max() - frame.min())\n",
    "                frame = self.transforms(frame)\n",
    "\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                frame = torch.zeros((CHANNELS, HEIGHT, WIDTH))\n",
    "\n",
    "            video[e,:,:,:] = frame\n",
    "          \n",
    "        return video, torch.LongTensor(self.y[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tt_split(data_folder, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    n = 0\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if(filename[-3:]==\"csv\"):\n",
    "            df = pd.read_csv(osp.join(data_folder,filename))\n",
    "            n += 1\n",
    "            for i in range(len(df)-seq_len):\n",
    "                X.append(df['frames'][i:i+seq_len].tolist())\n",
    "                y.append(df['labels'][i:i+seq_len].tolist())\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    # print(X_train)\n",
    "    # print(n)\n",
    "    # print(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xNuSoj1BLWea",
    "outputId": "a1e98128-490a-4651-c60f-ef4499290378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe can also explore https://github.com/okankop/vidaug for video based augmentations.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can also explore https://github.com/okankop/vidaug for video based augmentations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IX1zg-g3In-9",
    "outputId": "aff9d5b2-0a91-4ecc-b6b2-9beaed1448aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# train_transforms = [ttf.ToTensor(), transforms.Resize((HEIGHT, WIDTH)), transforms.ColorJitter(), transforms.RandomRotation(10), transforms.GaussianBlur(3)]\n",
    "train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((HEIGHT, WIDTH))])\n",
    "val_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((HEIGHT, WIDTH))])\n",
    "\n",
    "X_train, X_test, y_train, y_test = make_tt_split(DATA_SAVE_PATH, seq_len = SEQUENCE_LENGTH)\n",
    "train_dataset = VideoDataset(X_train, y_train, transforms=train_transforms, seq_len = SEQUENCE_LENGTH, base_path = PROCESSED_PATH)\n",
    "val_dataset = VideoDataset(X_test, y_test, transforms=val_transforms, seq_len = SEQUENCE_LENGTH, base_path = PROCESSED_PATH)\n",
    "\n",
    "\n",
    "train_args = dict(shuffle=True, batch_size=BATCH, num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=True, batch_size=BATCH, drop_last=False)\n",
    "train_loader = DataLoader(train_dataset, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=BATCH, num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=BATCH, drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, **val_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tgnvcg7a8hHp",
    "outputId": "a4373e9f-1c67-477f-d9a7-4f3088d160cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5432\n",
      "1358\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b5YXDoksGA_i"
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h) #[batch_size, self.hidden_dim, height, width]\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1) #[batch_size,t,self.hidden_dim, height, width]\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9_s93Au3Rk-F"
   },
   "outputs": [],
   "source": [
    "class ConvLSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False, num_classes = 3):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dim, kernel_size, num_layers,batch_first, bias, return_all_layers)\n",
    "        self.linear = nn.Linear(hidden_dim * HEIGHT * WIDTH, num_classes)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "      x,_ = self.convlstm(input_tensor)\n",
    "      # print(x[0].shape)  # torch.Size([2, 8, 128, 256, 256])\n",
    "      x = torch.flatten(x[0], start_dim=2)\n",
    "      # print(x.shape)  \t# torch.Size([2, 8, 8388608])\n",
    "      x = self.linear(x) #op: [batch, t, num_classes]\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UrI2yNxqbvvI"
   },
   "outputs": [],
   "source": [
    "def save(model, index, optim = False):\n",
    "    if not os.path.exists(MODELS_PATHS+'/attempt3_1sec_prior'):\n",
    "        os.mkdir(MODELS_PATHS+'/attempt3_1sec_prior')\n",
    "    if(optim):\n",
    "        torch.save(model.state_dict(), MODELS_PATHS+'/attempt3_1sec_prior'+'/optimizer_params_{:08d}.pth'.format(index))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), MODELS_PATHS+'/attempt3_1sec_prior'+'/model_params_{:08d}.pth'.format(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sl89a6wJMJEA",
    "outputId": "dbe36a2f-f49a-44f4-e87e-ddb88ab225bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTMModel(\n",
      "  (convlstm): ConvLSTM(\n",
      "    (cell_list): ModuleList(\n",
      "      (0): ConvLSTMCell(\n",
      "        (conv): Conv2d(131, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (1): ConvLSTMCell(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2097152, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.006 #changed from 0.01\n",
    "epochs = 25\n",
    "lamda = 1e-3  #L2 regularization #changed from 1e-4\n",
    "num_classes = 3\n",
    "convlstm_hidden = 128\n",
    "num_conv_lstm_layers = 2\n",
    "\n",
    "model = ConvLSTMModel(CHANNELS,convlstm_hidden,(3,3),num_conv_lstm_layers,True)\n",
    "model.load_state_dict(torch.load('./models/attempt3_1sec_prior/model_params_00000000.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lamda, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lamda)\n",
    "optimizer.load_state_dict(torch.load('./models/attempt3_1sec_prior/optimizer_params_00000000.pth'))\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = lr\n",
    "    g['weight_decay']= lamda\n",
    "    \n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "dhrrwnCikFAG",
    "outputId": "fb495db9-61a9-4511-b30c-ea4144f8004e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: Train Acc 79.5711%, Train Loss 0.5685, Learning Rate 0.0060\n",
      "Validation: 81.8115%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: Train Acc 79.5103%, Train Loss 0.5768, Learning Rate 0.0059\n",
      "Validation: 82.0029%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: Train Acc 78.7703%, Train Loss 1.1540, Learning Rate 0.0058\n",
      "Validation: 83.8586%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: Train Acc 79.6226%, Train Loss 1.0221, Learning Rate 0.0056\n",
      "Validation: 83.9028%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: Train Acc 78.7482%, Train Loss 0.7984, Learning Rate 0.0054\n",
      "Validation: 81.8041%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: Train Acc 80.1068%, Train Loss 0.6514, Learning Rate 0.0052\n",
      "Validation: 80.9499%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: Train Acc 79.9853%, Train Loss 1.0597, Learning Rate 0.0049\n",
      "Validation: 76.7673%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: Train Acc 77.7043%, Train Loss 0.7606, Learning Rate 0.0046\n",
      "Validation: 84.9043%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: Train Acc 82.6491%, Train Loss 0.5763, Learning Rate 0.0043\n",
      "Validation: 84.5361%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: Train Acc 82.2294%, Train Loss 0.7284, Learning Rate 0.0039\n",
      "Validation: 85.5817%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: Train Acc 83.8402%, Train Loss 0.5373, Learning Rate 0.0036\n",
      "Validation: 83.2916%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: Train Acc 83.8936%, Train Loss 0.6767, Learning Rate 0.0032\n",
      "Validation: 84.5950%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: Train Acc 84.7772%, Train Loss 0.4684, Learning Rate 0.0028\n",
      "Validation: 87.3490%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: Train Acc 86.2334%, Train Loss 0.4081, Learning Rate 0.0024\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,epochs):\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "       \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.float().to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(x)\n",
    "            del x\n",
    "            loss = criterion(outputs.view(-1,num_classes), y.long().view(-1))\n",
    "\n",
    "        num_correct += int((torch.argmax(outputs, axis=2) == y).sum())\n",
    "        del outputs\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * BATCH * SEQUENCE_LENGTH)),\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            num_correct=num_correct,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "        \n",
    "\n",
    "    batch_bar.close()\n",
    "\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "        epoch + 1,\n",
    "        epochs,\n",
    "        100 * num_correct / (len(train_loader) * BATCH * SEQUENCE_LENGTH),\n",
    "        float(total_loss / len(train_loader)),\n",
    "        float(optimizer.param_groups[0]['lr'])))\n",
    "    \n",
    "    save(model, epoch)\n",
    "    save(optimizer, epoch, optim=True)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_num_correct = 0\n",
    "   \n",
    "    for i, (vx, vy) in enumerate(val_loader):\n",
    "      \n",
    "      vx = vx.to(device)\n",
    "      vy = vy.to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          outputs = model(vx)\n",
    "          del vx\n",
    "\n",
    "      val_num_correct += int((torch.argmax(outputs, axis=2) == vy).sum())\n",
    "      del outputs\n",
    "\n",
    "    print(\"Validation: {:.04f}%\".format(100 * val_num_correct / (len(val_dataset) * SEQUENCE_LENGTH)))\n",
    "\n",
    "    \n",
    "batch_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AI_Guide_Dog_Training_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
