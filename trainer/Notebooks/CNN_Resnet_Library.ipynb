{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import yaml\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from utils import save as usave\n",
    "from torchvision import transforms\n",
    "from trainer.dataset import FrameDataset, CustomSampler\n",
    "from trainer.models import ResModel\n",
    "import pickle5 as pickle\n",
    "import pdb\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "def load_config():\n",
    "    with open(\"config_cnn.yaml\", \"r\") as configfile:\n",
    "        config_dict = yaml.load(configfile, Loader=yaml.FullLoader)\n",
    "    # print(config_dict)\n",
    "    return config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3w1l113h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>███▇▇▇▆▆▅▅▄▃▃▂▂▂▁▁▁▁</td></tr><tr><td>Test F1 0</td><td>▁</td></tr><tr><td>Test F1 1</td><td>▁</td></tr><tr><td>Test F1 2</td><td>▁</td></tr><tr><td>Test Precision 0</td><td>▁</td></tr><tr><td>Test Precision 1</td><td>▁</td></tr><tr><td>Test Precision 2</td><td>▁</td></tr><tr><td>Test Recall 0</td><td>▁</td></tr><tr><td>Test Recall 1</td><td>▁</td></tr><tr><td>Test Recall 2</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▅▆▆▇▇▇▇▇████████</td></tr><tr><td>Train Loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Val F1 0</td><td>▁▆▆▆▇█▇▇▆▆▇▆▆▆▆▆▅▅▆▅</td></tr><tr><td>Val F1 1</td><td>▁▅▇▅▇▇▇█▆▇▇▇▇▅▆▅▆▅▆▆</td></tr><tr><td>Val F1 2</td><td>▁▆▆▆▇▇▇███▇█████████</td></tr><tr><td>Val Precision 0</td><td>▁▄▄▄▅▇▇▇▇▇▇█▇▇▇▇██▇█</td></tr><tr><td>Val Precision 1</td><td>▁▃▄▄▅▅▅▇▆▅▅▆▇█▆▇▇▇▇▇</td></tr><tr><td>Val Precision 2</td><td>▇█▇▆▆▄▄▃▂▃▄▂▂▁▂▁▁▁▂▁</td></tr><tr><td>Val Recall 0</td><td>████▇▅▄▅▃▃▅▂▃▃▃▃▁▁▂▁</td></tr><tr><td>Val Recall 1</td><td>██▇▆▆▆▆▄▄▆▅▄▃▁▄▁▂▂▂▃</td></tr><tr><td>Val Recall 2</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇█▇█████</td></tr><tr><td>Validation Accuracy</td><td>▁▅▆▆▇▇▇██▇▇█████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.0</td></tr><tr><td>Test F1 0</td><td>0.49268</td></tr><tr><td>Test F1 1</td><td>0.55067</td></tr><tr><td>Test F1 2</td><td>0.78463</td></tr><tr><td>Test Precision 0</td><td>0.55891</td></tr><tr><td>Test Precision 1</td><td>0.53908</td></tr><tr><td>Test Precision 2</td><td>0.76658</td></tr><tr><td>Test Recall 0</td><td>0.44048</td></tr><tr><td>Test Recall 1</td><td>0.56276</td></tr><tr><td>Test Recall 2</td><td>0.80355</td></tr><tr><td>Train Accuracy</td><td>51.89474</td></tr><tr><td>Train Loss</td><td>0.01103</td></tr><tr><td>Val F1 0</td><td>0.49248</td></tr><tr><td>Val F1 1</td><td>0.56802</td></tr><tr><td>Val F1 2</td><td>0.78642</td></tr><tr><td>Val Precision 0</td><td>0.53333</td></tr><tr><td>Val Precision 1</td><td>0.5641</td></tr><tr><td>Val Precision 2</td><td>0.77427</td></tr><tr><td>Val Recall 0</td><td>0.45743</td></tr><tr><td>Val Recall 1</td><td>0.572</td></tr><tr><td>Val Recall 2</td><td>0.79896</td></tr><tr><td>Validation Accuracy</td><td>69.69759</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">library_cnn_resnet_undersampling_finetune</strong>: <a href=\"https://wandb.ai/anjadhav_cmu/AIGD/runs/3w1l113h\" target=\"_blank\">https://wandb.ai/anjadhav_cmu/AIGD/runs/3w1l113h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221203_050602-3w1l113h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3w1l113h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anjadhav/AIGD/AI_Guide_Dog/trainer/Notebooks/wandb/run-20221203_051835-20gnabef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anjadhav_cmu/AIGD/runs/20gnabef\" target=\"_blank\">library_cnn_resnet_undersampling_finetune_hyp</a></strong> to <a href=\"https://wandb.ai/anjadhav_cmu/AIGD\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'global': {'root_dir': '', 'iteration': 'library_cnn_resnet_undersampling_finetune_hyp', 'description': 'indoor_baseline full run NO intent, custom undersampling, finetuning resnet, changing hyperparams', 'enable_preprocessing': False, 'seed': 8, 'enable_intent': False, 'enable_wandb': True}, 'transformer': {'fps': 2, 'path': '../../data/train_rgb/', 'data_save_file': '../../data/train_rgb', 'enable_benchmark_test': True, 'test_path': '../../data/train_rgb/', 'test_save_file': '../../data/test_rgb'}, 'data': {'HEIGHT': 64, 'WIDTH': 64, 'CHANNELS': 3, 'SEQUENCE_LENGTH': 10, 'BENCHMARK_TEST_INTENT': ''}, 'trainer': {'BATCH': 32, 'lr': 1e-05, 'epochs': 10, 'lambda': 0.001, 'num_classes': 3, 'model_save_path': '../../models/', 'model': {'name': 'resnet', 'fixconvs': False, 'pretrained_path': '', 'optimizer_path': ''}}}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "config_dict = load_config()\n",
    "    \n",
    "if(config_dict['global']['enable_wandb']):\n",
    "    import wandb\n",
    "    wandb.init(name=config_dict['global']['iteration'], \n",
    "        project=\"AIGD\", \n",
    "        notes=config_dict['global']['description']) \n",
    "    #  , config=config_dict)\n",
    "else:\n",
    "    wandb = None\n",
    "\n",
    "print(config_dict)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "epochs = config_dict['trainer']['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos in the training file:  dict_keys(['2022-09-25T16:16:15', '2022-07-12T17-54-18', '2022-09-25T15:35:09', '2022-07-12T16-52-14', '2022-09-25T16:25:49', '2022-09-24T16:34:16', '2022-07-12T17-38-36', '2022-09-25T15:30:45', '2022-09-24T16:29:46', '2022-09-24T15:12:51', '2022-09-25T15:52:44', '2022-09-24T15:59:46', '2022-07-12T17-08-12', '2022-09-24T15:32:17', '2022-07-12T16-44-21', '2022-07-12T17-12-05', '2022-09-25T15:43:59', '2022-09-24T15:37:31', '2022-09-25T15:39:29', '2022-07-12T17-02-16', '2022-09-24T16:25:22', '2022-09-24T16:08:54', '2022-09-24T15:23:37', '2022-09-25T15:48:21', '2022-09-24T15:41:55', '2022-07-12T17-15-22', '2022-07-12T17-52-00', '2022-09-24T15:27:58', '2022-09-24T16:13:32', '2022-09-24T15:51:04', '2022-09-24T16:42:30', '2022-09-25T16:21:25', '2022-07-12T17-32-15', '2022-09-24T15:46:25', '2022-09-24T16:04:23', '2022-09-25T15:57:42', '2022-09-24T15:18:08', '2022-07-12T17-25-48', '2022-09-24T16:20:46', '2022-07-12T17-46-01'])\n",
      "Test Files:  dict_keys(['2022-09-24T15:55:29', '2022-07-12T16-34-07', '2022-09-24T16:46:51', '2022-09-24T15:08:27', '2022-07-12T17-42-18'])\n"
     ]
    }
   ],
   "source": [
    "df_videos = dict(np.load(config_dict['transformer']['data_save_file'] + '_video.npz', allow_pickle=True))\n",
    "print(\"Videos in the training file: \", df_videos.keys())\n",
    "\n",
    "# need video and sensor data separately\n",
    "with open(config_dict['transformer']['data_save_file'] + '_sensor.pickle', 'rb') as handle:\n",
    "    df_sensor = pickle.load(handle)\n",
    "\n",
    "if (config_dict['transformer']['enable_benchmark_test'] == True):\n",
    "    test_videos = dict(np.load(config_dict['transformer']['test_save_file'] + '_video.npz', allow_pickle=True))\n",
    "    with open(config_dict['transformer']['test_save_file'] + '_sensor.pickle', 'rb') as handle:\n",
    "        test_sensor = pickle.load(handle)\n",
    "    print(\"Test Files: \", test_videos.keys())\n",
    "\n",
    "else:\n",
    "    test_videos = None\n",
    "    test_sensor = None\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "val_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_files, val_files = make_tt_split(list(df_videos.keys()),config_dict['global']['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left count:  2169\n",
      "Right count:  2485\n",
      "Count of max class:  2485\n",
      "Examples to yield per epoch:  7139\n",
      "Length of val set:  4861\n",
      "Length of train set:  13643\n",
      "Length of test set:  2308\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FrameDataset(df_videos, df_sensor, sorted(train_files), transforms=train_transforms, config_dict=config_dict)\n",
    "val_dataset = FrameDataset(df_videos, df_sensor, sorted(val_files), transforms=val_transforms, config_dict=config_dict)\n",
    "\n",
    "sampler = CustomSampler(train_dataset, majority_percent=1)\n",
    "train_args = dict(sampler = sampler, batch_size=config_dict['trainer']['BATCH'], pin_memory=True, drop_last=False) if cuda else dict(batch_size=config_dict['trainer']['BATCH'], sampler = sampler, drop_last=False)\n",
    "train_loader = DataLoader(train_dataset, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=config_dict['trainer']['BATCH'], num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=config_dict['trainer']['BATCH'], drop_last=False)\n",
    "val_loader = DataLoader(val_dataset, **val_args)\n",
    "\n",
    "print(\"Length of val set: \", len(val_dataset))\n",
    "print(\"Length of train set: \", len(train_dataset))\n",
    "\n",
    "if config_dict['transformer']['enable_benchmark_test'] and test_videos is not None:\n",
    "    test_dataset = FrameDataset(test_videos, test_sensor, sorted(list(test_videos.keys())), transforms=val_transforms, config_dict=config_dict)\n",
    "    test_args = dict(shuffle=False, batch_size=config_dict['trainer']['BATCH'], num_workers=2, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=config_dict['trainer']['BATCH'], drop_last=False)\n",
    "    test_loader = DataLoader(test_dataset, **test_args)\n",
    "    print(\"Length of test set: \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Resnet Lol\n",
      "Not freezing params of ResNet\n",
      "ResModel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (regressor): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ResModel(fixconvs = config_dict['trainer']['model']['fixconvs'])\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "weights = [2.0,2.0,1.0]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config_dict['trainer']['lr'], weight_decay=config_dict['trainer']['lambda'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(epoch):\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "    \n",
    "    num_correct = 0.0\n",
    "    total_loss = 0.0\n",
    "    y_cnt = 0.0\n",
    "    actual = []\n",
    "    predictions = []\n",
    "    \n",
    "    for _, (x, y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # print(y.shape)\n",
    "        x = x.float().to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        del x\n",
    "        loss = criterion(outputs, y.long())\n",
    "\n",
    "        pred_class = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        actual.extend(y.detach().cpu())\n",
    "        predictions.extend(pred_class.detach().cpu())\n",
    "\n",
    "        num_correct += int((pred_class == y).sum())\n",
    "        del outputs\n",
    "        total_loss += (float(loss)*len(y))\n",
    "        y_cnt += len(y)\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            acc=\"{:.04f}%\".format(100 * float(num_correct) / y_cnt),\n",
    "            loss=\"{:.04f}\".format(float(total_loss) / y_cnt),\n",
    "            num_correct=num_correct,\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        \n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        batch_bar.update() # Update tqdm bar  \n",
    "              \n",
    "\n",
    "    batch_bar.close()\n",
    "    total_loss = float(total_loss) / len(train_dataset)\n",
    "    acc = 100 * float(num_correct) / (len(train_dataset))\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "        epoch + 1,\n",
    "        epochs,\n",
    "        acc,\n",
    "        float(total_loss),\n",
    "        float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "    if(wandb is not None):\n",
    "        wandb.log({\"Train Loss\": total_loss, \"Train Accuracy\": acc, \"Learning Rate\": float(optimizer.param_groups[0]['lr'])})\n",
    "\n",
    "\n",
    "    return actual, predictions\n",
    "\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    val_num_correct = 0\n",
    "\n",
    "    actual = []\n",
    "    predictions = []\n",
    "\n",
    "    \n",
    "    for _, (vx, vy) in tqdm(enumerate(val_loader)):\n",
    "    \n",
    "        vx = vx.to(device)\n",
    "        vy = vy.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(vx)\n",
    "            del vx\n",
    "\n",
    "        pred_class = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        actual.extend(vy.detach().cpu())\n",
    "        predictions.extend(pred_class.detach().cpu())\n",
    "\n",
    "        val_num_correct += int((pred_class == vy).sum())\n",
    "        \n",
    "        del outputs\n",
    "     \n",
    "        \n",
    "\n",
    "    acc = 100 * float(val_num_correct) / (len(val_dataset))\n",
    "    print(\"Validation: {:.04f}%\".format(acc))\n",
    "\n",
    "    if(wandb is not None):\n",
    "        wandb.log({\"Validation Accuracy\": acc})\n",
    "    \n",
    "    return acc, actual, predictions\n",
    "\n",
    "\n",
    "# runs benchmark test at the end (after train and validation)\n",
    "def test():\n",
    "    model.eval()\n",
    "    val_num_correct = 0\n",
    "\n",
    "    actual = []\n",
    "    predictions = []\n",
    "\n",
    "    for i, (vx, vy) in tqdm(enumerate(test_loader)):\n",
    "        vx = vx.to(device)\n",
    "        vy = vy.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(vx)\n",
    "            del vx\n",
    "\n",
    "        pred_class = torch.argmax(outputs, axis=1)\n",
    "\n",
    "        actual.extend(vy.detach().cpu())\n",
    "        predictions.extend(pred_class.detach().cpu())\n",
    "\n",
    "        val_num_correct += int((pred_class == vy).sum())\n",
    "        del outputs\n",
    "     \n",
    "        \n",
    "    acc = 100 * float(val_num_correct) / (len(test_dataset))\n",
    "    print(\"Benchmark test: {:.04f}%\".format(acc))\n",
    "\n",
    "    return acc, actual, predictions\n",
    "\n",
    "\n",
    "def trainer_save(acc, epoch):\n",
    "    usave(config_dict, model, epoch, acc, optim = False)\n",
    "    usave(config_dict, optimizer, epoch, acc, optim = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_save(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Acc 25.7128%, Train Loss 0.5182, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 92.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 48.6525%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1072  794  303]\n",
      " [ 634 1577  274]\n",
      " [ 701  925  859]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.49      0.47      2169\n",
      "           1       0.48      0.63      0.55      2485\n",
      "           2       0.60      0.35      0.44      2485\n",
      "\n",
      "    accuracy                           0.49      7139\n",
      "   macro avg       0.51      0.49      0.48      7139\n",
      "weighted avg       0.51      0.49      0.48      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 495  245   47]\n",
      " [ 256  700   44]\n",
      " [ 843 1061 1170]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.63      0.42       787\n",
      "           1       0.35      0.70      0.47      1000\n",
      "           2       0.93      0.38      0.54      3074\n",
      "\n",
      "    accuracy                           0.49      4861\n",
      "   macro avg       0.53      0.57      0.47      4861\n",
      "weighted avg       0.71      0.49      0.50      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Acc 33.3578%, Train Loss 0.3849, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 87.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 48.1794%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1582  479  108]\n",
      " [ 365 2002  118]\n",
      " [ 697  821  967]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.73      0.66      2169\n",
      "           1       0.61      0.81      0.69      2485\n",
      "           2       0.81      0.39      0.53      2485\n",
      "\n",
      "    accuracy                           0.64      7139\n",
      "   macro avg       0.67      0.64      0.63      7139\n",
      "weighted avg       0.67      0.64      0.62      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 533  219   35]\n",
      " [ 240  726   34]\n",
      " [ 869 1122 1083]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.68      0.44       787\n",
      "           1       0.35      0.73      0.47      1000\n",
      "           2       0.94      0.35      0.51      3074\n",
      "\n",
      "    accuracy                           0.48      4861\n",
      "   macro avg       0.54      0.59      0.47      4861\n",
      "weighted avg       0.72      0.48      0.49      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Acc 36.7954%, Train Loss 0.3214, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 91.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 55.2561%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1738  333   98]\n",
      " [ 235 2161   89]\n",
      " [ 639  725 1121]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73      2169\n",
      "           1       0.67      0.87      0.76      2485\n",
      "           2       0.86      0.45      0.59      2485\n",
      "\n",
      "    accuracy                           0.70      7139\n",
      "   macro avg       0.73      0.71      0.69      7139\n",
      "weighted avg       0.73      0.70      0.69      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 500  214   73]\n",
      " [ 184  752   64]\n",
      " [ 688  952 1434]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.64      0.46       787\n",
      "           1       0.39      0.75      0.52      1000\n",
      "           2       0.91      0.47      0.62      3074\n",
      "\n",
      "    accuracy                           0.55      4861\n",
      "   macro avg       0.56      0.62      0.53      4861\n",
      "weighted avg       0.72      0.55      0.57      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Acc 38.8844%, Train Loss 0.2813, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 93.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 55.9144%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1855  222   92]\n",
      " [ 188 2198   99]\n",
      " [ 605  628 1252]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77      2169\n",
      "           1       0.72      0.88      0.79      2485\n",
      "           2       0.87      0.50      0.64      2485\n",
      "\n",
      "    accuracy                           0.74      7139\n",
      "   macro avg       0.76      0.75      0.73      7139\n",
      "weighted avg       0.77      0.74      0.73      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 502  201   84]\n",
      " [ 164  774   62]\n",
      " [ 653  979 1442]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.64      0.48       787\n",
      "           1       0.40      0.77      0.52      1000\n",
      "           2       0.91      0.47      0.62      3074\n",
      "\n",
      "    accuracy                           0.56      4861\n",
      "   macro avg       0.56      0.63      0.54      4861\n",
      "weighted avg       0.72      0.56      0.58      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Acc 40.6802%, Train Loss 0.2472, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 93.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 56.4493%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1921  174   74]\n",
      " [ 128 2282   75]\n",
      " [ 563  575 1347]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.80      2169\n",
      "           1       0.75      0.92      0.83      2485\n",
      "           2       0.90      0.54      0.68      2485\n",
      "\n",
      "    accuracy                           0.78      7139\n",
      "   macro avg       0.80      0.78      0.77      7139\n",
      "weighted avg       0.80      0.78      0.77      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 560  153   74]\n",
      " [ 234  690   76]\n",
      " [ 768  812 1494]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.71      0.48       787\n",
      "           1       0.42      0.69      0.52      1000\n",
      "           2       0.91      0.49      0.63      3074\n",
      "\n",
      "    accuracy                           0.56      4861\n",
      "   macro avg       0.56      0.63      0.54      4861\n",
      "weighted avg       0.72      0.56      0.58      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Acc 41.3326%, Train Loss 0.2292, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 90.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 56.6550%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1948  143   78]\n",
      " [ 111 2305   69]\n",
      " [ 535  564 1386]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82      2169\n",
      "           1       0.77      0.93      0.84      2485\n",
      "           2       0.90      0.56      0.69      2485\n",
      "\n",
      "    accuracy                           0.79      7139\n",
      "   macro avg       0.81      0.79      0.78      7139\n",
      "weighted avg       0.81      0.79      0.78      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 491  209   87]\n",
      " [ 162  769   69]\n",
      " [ 645  935 1494]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.62      0.47       787\n",
      "           1       0.40      0.77      0.53      1000\n",
      "           2       0.91      0.49      0.63      3074\n",
      "\n",
      "    accuracy                           0.57      4861\n",
      "   macro avg       0.56      0.63      0.54      4861\n",
      "weighted avg       0.72      0.57      0.58      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Acc 42.4980%, Train Loss 0.2096, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 94.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 58.8356%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[1990  113   66]\n",
      " [  85 2334   66]\n",
      " [ 510  501 1474]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84      2169\n",
      "           1       0.79      0.94      0.86      2485\n",
      "           2       0.92      0.59      0.72      2485\n",
      "\n",
      "    accuracy                           0.81      7139\n",
      "   macro avg       0.83      0.82      0.81      7139\n",
      "weighted avg       0.83      0.81      0.80      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 478  213   96]\n",
      " [ 140  775   85]\n",
      " [ 585  882 1607]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.61      0.48       787\n",
      "           1       0.41      0.78      0.54      1000\n",
      "           2       0.90      0.52      0.66      3074\n",
      "\n",
      "    accuracy                           0.59      4861\n",
      "   macro avg       0.57      0.64      0.56      4861\n",
      "weighted avg       0.72      0.59      0.61      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Acc 42.8938%, Train Loss 0.1979, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 92.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 59.6997%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[2016   96   57]\n",
      " [  67 2343   75]\n",
      " [ 484  508 1493]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85      2169\n",
      "           1       0.80      0.94      0.86      2485\n",
      "           2       0.92      0.60      0.73      2485\n",
      "\n",
      "    accuracy                           0.82      7139\n",
      "   macro avg       0.83      0.82      0.81      7139\n",
      "weighted avg       0.84      0.82      0.81      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 525  161  101]\n",
      " [ 193  704  103]\n",
      " [ 664  737 1673]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.67      0.48       787\n",
      "           1       0.44      0.70      0.54      1000\n",
      "           2       0.89      0.54      0.68      3074\n",
      "\n",
      "    accuracy                           0.60      4861\n",
      "   macro avg       0.57      0.64      0.57      4861\n",
      "weighted avg       0.72      0.60      0.62      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Acc 43.5022%, Train Loss 0.1907, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 95.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 57.5602%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[2048   75   46]\n",
      " [  56 2364   65]\n",
      " [ 484  478 1523]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86      2169\n",
      "           1       0.81      0.95      0.88      2485\n",
      "           2       0.93      0.61      0.74      2485\n",
      "\n",
      "    accuracy                           0.83      7139\n",
      "   macro avg       0.84      0.84      0.83      7139\n",
      "weighted avg       0.85      0.83      0.82      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 493  185  109]\n",
      " [ 170  730  100]\n",
      " [ 662  837 1575]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.63      0.47       787\n",
      "           1       0.42      0.73      0.53      1000\n",
      "           2       0.88      0.51      0.65      3074\n",
      "\n",
      "    accuracy                           0.58      4861\n",
      "   macro avg       0.56      0.62      0.55      4861\n",
      "weighted avg       0.70      0.58      0.59      4861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Acc 43.1797%, Train Loss 0.1905, Learning Rate 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:01, 91.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: 57.2927%\n",
      "\n",
      "Taining set stats\n",
      "\n",
      "[[2037   75   57]\n",
      " [  69 2346   70]\n",
      " [ 482  495 1508]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86      2169\n",
      "           1       0.80      0.94      0.87      2485\n",
      "           2       0.92      0.61      0.73      2485\n",
      "\n",
      "    accuracy                           0.83      7139\n",
      "   macro avg       0.84      0.83      0.82      7139\n",
      "weighted avg       0.84      0.83      0.82      7139\n",
      "\n",
      "\n",
      "Validation set stats\n",
      "\n",
      "[[ 503  190   94]\n",
      " [ 186  734   80]\n",
      " [ 675  851 1548]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.64      0.47       787\n",
      "           1       0.41      0.73      0.53      1000\n",
      "           2       0.90      0.50      0.65      3074\n",
      "\n",
      "    accuracy                           0.57      4861\n",
      "   macro avg       0.56      0.63      0.55      4861\n",
      "weighted avg       0.71      0.57      0.59      4861\n",
      "\n",
      "Completed Training!!\n",
      "Starting benchmark testing!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:00, 79.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark test: 56.2825%\n",
      "\n",
      "Test set stats\n",
      "\n",
      "[[267 105  48]\n",
      " [ 82 331  65]\n",
      " [338 371 701]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.64      0.48       420\n",
      "           1       0.41      0.69      0.52       478\n",
      "           2       0.86      0.50      0.63      1410\n",
      "\n",
      "    accuracy                           0.56      2308\n",
      "   macro avg       0.55      0.61      0.54      2308\n",
      "weighted avg       0.68      0.56      0.58      2308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_actual, train_predictions = train(epoch)\n",
    "    acc, val_actual, val_predictions = validate()\n",
    "    val_precision, val_recall, val_f1, _ = display_classification_report(train_actual, train_predictions, val_actual, val_predictions)\n",
    "    \n",
    "    if(config_dict['global']['enable_wandb']):\n",
    "        wandb.log({\"Val Precision 0\": val_precision[0], \"Val Precision 1\": val_precision[1], \"Val Precision 2\": val_precision[2]})\n",
    "        wandb.log({\"Val Recall 0\": val_recall[0], \"Val Recall 1\": val_recall[1], \"Val Recall 2\": val_recall[2]})\n",
    "        wandb.log({\"Val F1 0\": val_f1[0], \"Val F1 1\": val_f1[1], \"Val F1 2\": val_f1[2]})\n",
    "\n",
    "    trainer_save(acc, epoch)\n",
    "    \n",
    "\n",
    "print(\"Completed Training!!\")\n",
    "# performs final benchmarking after training\n",
    "if (config_dict['transformer']['enable_benchmark_test'] == True):\n",
    "    print(\"Starting benchmark testing!!\")\n",
    "    acc, test_actual, test_predictions = test()\n",
    "    test_precision, test_recall, test_f1, _ = display_test_classification_report(test_actual, test_predictions)\n",
    "    if(config_dict['global']['enable_wandb']):\n",
    "        wandb.log({\"Test Precision 0\": test_precision[0], \"Test Precision 1\": test_precision[1], \"Test Precision 2\": test_precision[2]})\n",
    "        wandb.log({\"Test Recall 0\": test_recall[0], \"Test Recall 1\": test_recall[1], \"Test Recall 2\": test_recall[2]})\n",
    "        wandb.log({\"Test F1 0\": test_f1[0], \"Test F1 1\": test_f1[1], \"Test F1 2\": test_f1[2]})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('chemIR')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bca01836d6106e973a10514ba84a22def982a0caf4526f342e9e0f31ef9581b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
